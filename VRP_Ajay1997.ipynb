{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ajayn1997"
      ],
      "metadata": {
        "id": "AwwHXfBOrJon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNIvEccWtBkx",
        "outputId": "2cc0dc4f-3bf6-4b49-8644-161afeda2f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mâœ¨ðŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install pytorch=0.4.1 cuda75 -c pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PttmyU6Azkul",
        "outputId": "8f0409db-e427-4378-80eb-6b38d7e1222e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n",
            "Retrieving notices: ...working... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXNC-VQZkLj8",
        "outputId": "07567883-8663-4095-99ba-4d914793d7ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'RL-VRP-PtrNtwrk' already exists and is not an empty directory.\n",
            "condacolab_install.log\tRL-VRP-PtrNtwrk  sample_data\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ajayn1997/RL-VRP-PtrNtwrk.git\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd RL-VRP-PtrNtwrk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gHbnkMLks0m",
        "outputId": "c9dcfbe6-48b3-40cf-cfbb-d1b8bfd6cecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RL-VRP-PtrNtwrk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiW4X7fJkyws",
        "outputId": "9ebb6cf0-bbfc-46b9-fb67-24d809ed95b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main.py  Models  README.md  Tasks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47AISstI1KIW",
        "outputId": "1bda69bc-e48d-491f-bd99-e7c38e255424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/site-packages (3.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib) (9.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/site-packages (from matplotlib) (4.37.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/site-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment line 267,268,269"
      ],
      "metadata": {
        "id": "H0XS43IFGToo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comment the line 225 to 230 to train the model"
      ],
      "metadata": {
        "id": "gC_ro52_zL7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update LOAD_DICT in line 186 in main.py file to run for 5 nodes"
      ],
      "metadata": {
        "id": "5fmRfr_ieyWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--train-size=50000 --nodes=10 time 26 min"
      ],
      "metadata": {
        "id": "Qr_rXoJuQHWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 main.py --train-size=50000 --nodes=10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwqWGvjVlI3n",
        "outputId": "ca2ac1b2-43f1-47ba-d62c-645dcc413b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected device cuda\n",
            "NOTE: SETTTING CHECKPOINT: \n",
            "vrp/10/12_59_47.350165/\n",
            "Starting VRP training\n",
            "Train data: <Tasks.vrp.VehicleRoutingDataset object at 0x7f118869af90>\n",
            "Actor: DRL4TSP(\n",
            "  (static_encoder): Encoder(\n",
            "    (conv): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (dynamic_encoder): Encoder(\n",
            "    (conv): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (decoder): Encoder(\n",
            "    (conv): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (pointer): Pointer(\n",
            "    (gru): GRU(128, 128, batch_first=True)\n",
            "    (encoder_attn): Attention()\n",
            "    (drop_rnn): Dropout(p=0.1)\n",
            "    (drop_hh): Dropout(p=0.1)\n",
            "  )\n",
            ") \n",
            "Critic: StateCritic(\n",
            "  (static_encoder): Encoder(\n",
            "    (conv): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (dynamic_encoder): Encoder(\n",
            "    (conv): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n",
            "  )\n",
            "  (fc1): Conv1d(256, 20, kernel_size=(1,), stride=(1,))\n",
            "  (fc2): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n",
            "  (fc3): Conv1d(20, 1, kernel_size=(1,), stride=(1,))\n",
            ")\n",
            "Starting training\n",
            "  Batch 99/1954, reward: 7.409, loss: -50.5660, took: 36.5834s\n",
            "  Batch 199/1954, reward: 7.396, loss: -0.5032, took: 36.2154s\n",
            "  Batch 299/1954, reward: 7.356, loss: -0.2371, took: 38.4471s\n",
            "  Batch 399/1954, reward: 7.139, loss: -0.8354, took: 36.1832s\n",
            "  Batch 499/1954, reward: 6.808, loss: -1.2657, took: 35.8499s\n",
            "  Batch 599/1954, reward: 6.619, loss: -0.5988, took: 35.0375s\n",
            "  Batch 699/1954, reward: 6.484, loss: -0.6490, took: 34.4793s\n",
            "  Batch 799/1954, reward: 6.383, loss: -0.4972, took: 36.6149s\n",
            "  Batch 899/1954, reward: 6.339, loss: -0.4205, took: 33.9517s\n",
            "  Batch 999/1954, reward: 6.296, loss: -0.1683, took: 33.8341s\n",
            "  Batch 1099/1954, reward: 6.219, loss: -0.1005, took: 33.7796s\n",
            "  Batch 1199/1954, reward: 6.159, loss: 0.1113, took: 33.6561s\n",
            "  Batch 1299/1954, reward: 6.103, loss: 0.0075, took: 33.6447s\n",
            "  Batch 1399/1954, reward: 6.049, loss: 0.0062, took: 36.0255s\n",
            "  Batch 1499/1954, reward: 6.032, loss: 0.1410, took: 33.9848s\n",
            "  Batch 1599/1954, reward: 6.006, loss: 0.1193, took: 34.0299s\n",
            "  Batch 1699/1954, reward: 5.977, loss: 0.1171, took: 33.8637s\n",
            "  Batch 1799/1954, reward: 5.952, loss: 0.1307, took: 33.8351s\n",
            "  Batch 1899/1954, reward: 5.957, loss: 0.2024, took: 34.0981s\n",
            "Mean epoch loss/reward: -2.8080, 6.4422, 5.8502, took: 689.7408s (34.9534s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.948, loss: 0.2821, took: 34.0838s\n",
            "  Batch 199/1954, reward: 5.926, loss: 0.1836, took: 34.0327s\n",
            "  Batch 299/1954, reward: 5.912, loss: 0.1620, took: 33.9147s\n",
            "  Batch 399/1954, reward: 5.915, loss: 0.1077, took: 33.9313s\n",
            "  Batch 499/1954, reward: 5.899, loss: 0.1952, took: 33.6236s\n",
            "  Batch 599/1954, reward: 5.885, loss: 0.2061, took: 35.5772s\n",
            "  Batch 699/1954, reward: 5.902, loss: 0.1333, took: 33.8387s\n",
            "  Batch 799/1954, reward: 5.895, loss: 0.1921, took: 33.9039s\n",
            "  Batch 899/1954, reward: 5.878, loss: 0.1689, took: 33.8733s\n",
            "  Batch 999/1954, reward: 5.877, loss: 0.1612, took: 34.3827s\n",
            "  Batch 1099/1954, reward: 5.873, loss: 0.3122, took: 36.2469s\n",
            "  Batch 1199/1954, reward: 5.859, loss: 0.2388, took: 33.5745s\n",
            "  Batch 1299/1954, reward: 5.866, loss: 0.2359, took: 33.6646s\n",
            "  Batch 1399/1954, reward: 5.865, loss: 0.1941, took: 33.7053s\n",
            "  Batch 1499/1954, reward: 5.855, loss: 0.1123, took: 33.6117s\n",
            "  Batch 1599/1954, reward: 5.849, loss: 0.1420, took: 33.4655s\n",
            "  Batch 1699/1954, reward: 5.832, loss: 0.2620, took: 35.1508s\n",
            "  Batch 1799/1954, reward: 5.855, loss: 0.1893, took: 33.4746s\n",
            "  Batch 1899/1954, reward: 5.846, loss: 0.1993, took: 33.5898s\n",
            "Mean epoch loss/reward: 0.1881, 5.8799, 5.7755, took: 670.5915s (34.0866s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.840, loss: 0.1729, took: 33.5625s\n",
            "  Batch 199/1954, reward: 5.840, loss: 0.1931, took: 33.5769s\n",
            "  Batch 299/1954, reward: 5.827, loss: 0.2991, took: 35.2024s\n",
            "  Batch 399/1954, reward: 5.842, loss: 0.2611, took: 33.0816s\n",
            "  Batch 499/1954, reward: 5.844, loss: 0.1771, took: 33.1282s\n",
            "  Batch 599/1954, reward: 5.838, loss: 0.1610, took: 33.4189s\n",
            "  Batch 699/1954, reward: 5.827, loss: 0.2677, took: 33.2679s\n",
            "  Batch 799/1954, reward: 5.835, loss: 0.2188, took: 33.1304s\n",
            "  Batch 899/1954, reward: 5.820, loss: 0.2375, took: 35.1640s\n",
            "  Batch 999/1954, reward: 5.820, loss: 0.2722, took: 32.8076s\n",
            "  Batch 1099/1954, reward: 5.819, loss: 0.2786, took: 32.9680s\n",
            "  Batch 1199/1954, reward: 5.809, loss: 0.1868, took: 33.1387s\n",
            "  Batch 1299/1954, reward: 5.815, loss: 0.2134, took: 33.3899s\n",
            "  Batch 1399/1954, reward: 5.824, loss: 0.1482, took: 33.4274s\n",
            "  Batch 1499/1954, reward: 5.818, loss: 0.1928, took: 35.4965s\n",
            "  Batch 1599/1954, reward: 5.820, loss: 0.2379, took: 33.3603s\n",
            "  Batch 1699/1954, reward: 5.804, loss: 0.1851, took: 32.9032s\n",
            "  Batch 1799/1954, reward: 5.807, loss: 0.1643, took: 33.2898s\n",
            "  Batch 1899/1954, reward: 5.809, loss: 0.1314, took: 33.2549s\n",
            "Mean epoch loss/reward: 0.2101, 5.8234, 5.7209, took: 660.4212s (33.5563s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.801, loss: 0.1619, took: 35.5385s\n",
            "  Batch 199/1954, reward: 5.804, loss: 0.1734, took: 32.7796s\n",
            "  Batch 299/1954, reward: 5.816, loss: 0.1571, took: 32.8465s\n",
            "  Batch 399/1954, reward: 5.797, loss: 0.1902, took: 32.8759s\n",
            "  Batch 499/1954, reward: 5.797, loss: 0.1892, took: 33.0494s\n",
            "  Batch 599/1954, reward: 5.797, loss: 0.1733, took: 33.1680s\n",
            "  Batch 699/1954, reward: 5.802, loss: 0.2019, took: 34.7752s\n",
            "  Batch 799/1954, reward: 5.780, loss: 0.1972, took: 33.1112s\n",
            "  Batch 899/1954, reward: 5.792, loss: 0.1534, took: 33.2297s\n",
            "  Batch 999/1954, reward: 5.808, loss: 0.1458, took: 32.6021s\n",
            "  Batch 1099/1954, reward: 5.796, loss: 0.1653, took: 33.0448s\n",
            "  Batch 1199/1954, reward: 5.790, loss: 0.1352, took: 33.4823s\n",
            "  Batch 1299/1954, reward: 5.785, loss: 0.2106, took: 34.8712s\n",
            "  Batch 1399/1954, reward: 5.789, loss: 0.1963, took: 32.8555s\n",
            "  Batch 1499/1954, reward: 5.784, loss: 0.1326, took: 33.1838s\n",
            "  Batch 1599/1954, reward: 5.793, loss: 0.1936, took: 32.9341s\n",
            "  Batch 1699/1954, reward: 5.777, loss: 0.1597, took: 32.8573s\n",
            "  Batch 1799/1954, reward: 5.768, loss: 0.1401, took: 32.5145s\n",
            "  Batch 1899/1954, reward: 5.758, loss: 0.1820, took: 33.0570s\n",
            "Mean epoch loss/reward: 0.1723, 5.7904, 5.7069, took: 657.2407s (33.3040s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.764, loss: 0.1650, took: 33.4622s\n",
            "  Batch 199/1954, reward: 5.745, loss: 0.1712, took: 32.8806s\n",
            "  Batch 299/1954, reward: 5.751, loss: 0.1554, took: 32.7691s\n",
            "  Batch 399/1954, reward: 5.757, loss: 0.0633, took: 33.0605s\n",
            "  Batch 499/1954, reward: 5.750, loss: 0.1048, took: 32.9041s\n",
            "  Batch 599/1954, reward: 5.738, loss: 0.1357, took: 34.8210s\n",
            "  Batch 699/1954, reward: 5.753, loss: 0.1573, took: 32.6433s\n",
            "  Batch 799/1954, reward: 5.745, loss: 0.1328, took: 33.0200s\n",
            "  Batch 899/1954, reward: 5.747, loss: 0.1049, took: 32.7899s\n",
            "  Batch 999/1954, reward: 5.749, loss: 0.0794, took: 32.4286s\n",
            "  Batch 1099/1954, reward: 5.749, loss: 0.0537, took: 33.0228s\n",
            "  Batch 1199/1954, reward: 5.754, loss: 0.1107, took: 34.7823s\n",
            "  Batch 1299/1954, reward: 5.748, loss: -0.0147, took: 32.8987s\n",
            "  Batch 1399/1954, reward: 5.744, loss: 0.0476, took: 33.1538s\n",
            "  Batch 1499/1954, reward: 5.737, loss: 0.0373, took: 32.3420s\n",
            "  Batch 1599/1954, reward: 5.751, loss: 0.0703, took: 32.5503s\n",
            "  Batch 1699/1954, reward: 5.738, loss: 0.2128, took: 33.0411s\n",
            "  Batch 1799/1954, reward: 5.737, loss: 0.1003, took: 33.6907s\n",
            "  Batch 1899/1954, reward: 5.728, loss: 0.0956, took: 33.4897s\n",
            "Mean epoch loss/reward: 0.1044, 5.7459, 5.6835, took: 652.2383s (33.1448s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.728, loss: 0.0519, took: 32.6362s\n",
            "  Batch 199/1954, reward: 5.734, loss: 0.0946, took: 32.5988s\n",
            "  Batch 299/1954, reward: 5.730, loss: 0.0568, took: 33.1212s\n",
            "  Batch 399/1954, reward: 5.732, loss: 0.1640, took: 32.7715s\n",
            "  Batch 499/1954, reward: 5.740, loss: 0.0536, took: 34.8344s\n",
            "  Batch 599/1954, reward: 5.726, loss: 0.0726, took: 32.6052s\n",
            "  Batch 699/1954, reward: 5.730, loss: 0.1020, took: 32.6470s\n",
            "  Batch 799/1954, reward: 5.720, loss: 0.0795, took: 32.7458s\n",
            "  Batch 899/1954, reward: 5.718, loss: 0.0587, took: 32.4001s\n",
            "  Batch 999/1954, reward: 5.722, loss: 0.0953, took: 32.8194s\n",
            "  Batch 1099/1954, reward: 5.729, loss: 0.0481, took: 33.1154s\n",
            "  Batch 1199/1954, reward: 5.727, loss: 0.0233, took: 33.9215s\n",
            "  Batch 1299/1954, reward: 5.737, loss: 0.0897, took: 32.6214s\n",
            "  Batch 1399/1954, reward: 5.733, loss: 0.0573, took: 32.4047s\n",
            "  Batch 1499/1954, reward: 5.729, loss: 0.0080, took: 32.7574s\n",
            "  Batch 1599/1954, reward: 5.718, loss: 0.0591, took: 32.3460s\n",
            "  Batch 1699/1954, reward: 5.728, loss: -0.0064, took: 32.3108s\n",
            "  Batch 1799/1954, reward: 5.715, loss: 0.0652, took: 34.6313s\n",
            "  Batch 1899/1954, reward: 5.707, loss: 0.0720, took: 32.2444s\n",
            "Mean epoch loss/reward: 0.0682, 5.7257, 5.6739, took: 647.7189s (32.9228s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.710, loss: 0.1097, took: 32.4378s\n",
            "  Batch 199/1954, reward: 5.728, loss: 0.1159, took: 32.4216s\n",
            "  Batch 299/1954, reward: 5.713, loss: 0.0900, took: 32.8832s\n",
            "  Batch 399/1954, reward: 5.711, loss: 0.0386, took: 33.0805s\n",
            "  Batch 499/1954, reward: 5.717, loss: 0.0267, took: 34.8631s\n",
            "  Batch 599/1954, reward: 5.720, loss: 0.0294, took: 32.7494s\n",
            "  Batch 699/1954, reward: 5.731, loss: 0.0562, took: 32.4180s\n",
            "  Batch 799/1954, reward: 5.712, loss: 0.0784, took: 32.2649s\n",
            "  Batch 899/1954, reward: 5.708, loss: 0.0653, took: 32.2660s\n",
            "  Batch 999/1954, reward: 5.712, loss: 0.0883, took: 32.4007s\n",
            "  Batch 1099/1954, reward: 5.701, loss: 0.0156, took: 34.4721s\n",
            "  Batch 1199/1954, reward: 5.692, loss: 0.0534, took: 32.1411s\n",
            "  Batch 1299/1954, reward: 5.692, loss: 0.0714, took: 32.2897s\n",
            "  Batch 1399/1954, reward: 5.711, loss: 0.0653, took: 32.4848s\n",
            "  Batch 1499/1954, reward: 5.693, loss: 0.0586, took: 32.4627s\n",
            "  Batch 1599/1954, reward: 5.709, loss: 0.0178, took: 32.5355s\n",
            "  Batch 1699/1954, reward: 5.701, loss: 0.1001, took: 32.2748s\n",
            "  Batch 1799/1954, reward: 5.704, loss: 0.0505, took: 34.7338s\n",
            "  Batch 1899/1954, reward: 5.708, loss: 0.0365, took: 32.5460s\n",
            "Mean epoch loss/reward: 0.0626, 5.7097, 5.6566, took: 646.0280s (32.8277s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.711, loss: 0.0719, took: 32.5340s\n",
            "  Batch 199/1954, reward: 5.694, loss: 0.0448, took: 32.3218s\n",
            "  Batch 299/1954, reward: 5.699, loss: 0.0339, took: 32.6684s\n",
            "  Batch 399/1954, reward: 5.699, loss: 0.0526, took: 32.5711s\n",
            "  Batch 499/1954, reward: 5.707, loss: 0.0698, took: 34.7550s\n",
            "  Batch 599/1954, reward: 5.706, loss: 0.0808, took: 32.7373s\n",
            "  Batch 699/1954, reward: 5.692, loss: 0.1034, took: 32.2522s\n",
            "  Batch 799/1954, reward: 5.685, loss: 0.0237, took: 32.2646s\n",
            "  Batch 899/1954, reward: 5.707, loss: 0.0421, took: 32.1621s\n",
            "  Batch 999/1954, reward: 5.691, loss: 0.1297, took: 32.3945s\n",
            "  Batch 1099/1954, reward: 5.709, loss: 0.0807, took: 34.2842s\n",
            "  Batch 1199/1954, reward: 5.700, loss: 0.0630, took: 32.1348s\n",
            "  Batch 1299/1954, reward: 5.684, loss: 0.0505, took: 32.2790s\n",
            "  Batch 1399/1954, reward: 5.701, loss: 0.0733, took: 32.0878s\n",
            "  Batch 1499/1954, reward: 5.676, loss: 0.0373, took: 32.0772s\n",
            "  Batch 1599/1954, reward: 5.676, loss: 0.1022, took: 32.2660s\n",
            "  Batch 1699/1954, reward: 5.663, loss: 0.0941, took: 32.2115s\n",
            "  Batch 1799/1954, reward: 5.660, loss: 0.1095, took: 34.3326s\n",
            "  Batch 1899/1954, reward: 5.665, loss: 0.0603, took: 32.3054s\n",
            "Mean epoch loss/reward: 0.0699, 5.6901, 5.6024, took: 642.8690s (32.6652s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.680, loss: 0.1063, took: 32.5516s\n",
            "  Batch 199/1954, reward: 5.669, loss: 0.0812, took: 32.3246s\n",
            "  Batch 299/1954, reward: 5.657, loss: 0.0457, took: 32.3351s\n",
            "  Batch 399/1954, reward: 5.653, loss: 0.0236, took: 32.4241s\n",
            "  Batch 499/1954, reward: 5.661, loss: 0.1131, took: 34.4551s\n",
            "  Batch 599/1954, reward: 5.660, loss: 0.0927, took: 32.6429s\n",
            "  Batch 699/1954, reward: 5.649, loss: 0.0711, took: 32.4114s\n",
            "  Batch 799/1954, reward: 5.649, loss: 0.1128, took: 32.3542s\n",
            "  Batch 899/1954, reward: 5.660, loss: 0.0538, took: 32.8154s\n",
            "  Batch 999/1954, reward: 5.658, loss: 0.0830, took: 32.7697s\n",
            "  Batch 1099/1954, reward: 5.662, loss: 0.0876, took: 33.7348s\n",
            "  Batch 1199/1954, reward: 5.660, loss: 0.1055, took: 32.9933s\n",
            "  Batch 1299/1954, reward: 5.645, loss: -0.0042, took: 32.9458s\n",
            "  Batch 1399/1954, reward: 5.648, loss: 0.0969, took: 32.4308s\n",
            "  Batch 1499/1954, reward: 5.655, loss: 0.0446, took: 32.3098s\n",
            "  Batch 1599/1954, reward: 5.685, loss: -0.0218, took: 32.5302s\n",
            "  Batch 1699/1954, reward: 5.685, loss: 0.0440, took: 32.5366s\n",
            "  Batch 1799/1954, reward: 5.684, loss: 0.0556, took: 34.5821s\n",
            "  Batch 1899/1954, reward: 5.682, loss: 0.0366, took: 32.7254s\n",
            "Mean epoch loss/reward: 0.0662, 5.6634, 5.6330, took: 646.2399s (32.8354s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.670, loss: 0.0760, took: 33.9950s\n",
            "  Batch 199/1954, reward: 5.666, loss: 0.0723, took: 33.5603s\n",
            "  Batch 299/1954, reward: 5.660, loss: -0.0232, took: 33.2191s\n",
            "  Batch 399/1954, reward: 5.651, loss: 0.0291, took: 33.0700s\n",
            "  Batch 499/1954, reward: 5.633, loss: 0.0842, took: 35.4573s\n",
            "  Batch 599/1954, reward: 5.636, loss: 0.0263, took: 32.9810s\n",
            "  Batch 699/1954, reward: 5.632, loss: 0.1219, took: 33.3313s\n",
            "  Batch 799/1954, reward: 5.644, loss: 0.1006, took: 33.2723s\n",
            "  Batch 899/1954, reward: 5.639, loss: 0.0243, took: 33.0762s\n",
            "  Batch 999/1954, reward: 5.631, loss: -0.0093, took: 33.4742s\n",
            "  Batch 1099/1954, reward: 5.635, loss: -0.0087, took: 33.5090s\n",
            "  Batch 1199/1954, reward: 5.625, loss: 0.0354, took: 35.9816s\n",
            "  Batch 1299/1954, reward: 5.640, loss: 0.0734, took: 33.3912s\n",
            "  Batch 1399/1954, reward: 5.633, loss: 0.0690, took: 32.9552s\n",
            "  Batch 1499/1954, reward: 5.626, loss: 0.0775, took: 32.9202s\n",
            "  Batch 1599/1954, reward: 5.646, loss: -0.0049, took: 32.7915s\n",
            "  Batch 1699/1954, reward: 5.633, loss: 0.0711, took: 32.9214s\n",
            "  Batch 1799/1954, reward: 5.641, loss: 0.0361, took: 35.3341s\n",
            "  Batch 1899/1954, reward: 5.630, loss: 0.0727, took: 33.1096s\n",
            "Mean epoch loss/reward: 0.0470, 5.6404, 5.5753, took: 661.0844s (33.5974s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.625, loss: 0.0654, took: 33.0448s\n",
            "  Batch 199/1954, reward: 5.629, loss: 0.0728, took: 32.8075s\n",
            "  Batch 299/1954, reward: 5.638, loss: 0.0655, took: 32.7299s\n",
            "  Batch 399/1954, reward: 5.670, loss: 0.1216, took: 32.6279s\n",
            "  Batch 499/1954, reward: 5.653, loss: 0.1017, took: 35.0989s\n",
            "  Batch 599/1954, reward: 5.643, loss: 0.0313, took: 32.7781s\n",
            "  Batch 699/1954, reward: 5.653, loss: 0.0013, took: 32.5468s\n",
            "  Batch 799/1954, reward: 5.621, loss: 0.0196, took: 32.5833s\n",
            "  Batch 899/1954, reward: 5.635, loss: 0.0127, took: 32.4417s\n",
            "  Batch 999/1954, reward: 5.622, loss: -0.0052, took: 32.8265s\n",
            "  Batch 1099/1954, reward: 5.624, loss: -0.0270, took: 32.9271s\n",
            "  Batch 1199/1954, reward: 5.621, loss: -0.0354, took: 35.3488s\n",
            "  Batch 1299/1954, reward: 5.645, loss: 0.0066, took: 32.9462s\n",
            "  Batch 1399/1954, reward: 5.684, loss: -0.0491, took: 33.0213s\n",
            "  Batch 1499/1954, reward: 5.678, loss: -0.0484, took: 32.8587s\n",
            "  Batch 1599/1954, reward: 5.676, loss: 0.0009, took: 32.6179s\n",
            "  Batch 1699/1954, reward: 5.664, loss: -0.0162, took: 32.9672s\n",
            "  Batch 1799/1954, reward: 5.679, loss: -0.0354, took: 33.6516s\n",
            "  Batch 1899/1954, reward: 5.653, loss: 0.0442, took: 34.3255s\n",
            "Mean epoch loss/reward: 0.0160, 5.6474, 5.5664, took: 653.0362s (33.1658s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.629, loss: -0.0131, took: 32.9498s\n",
            "  Batch 199/1954, reward: 5.642, loss: -0.0536, took: 32.6132s\n",
            "  Batch 299/1954, reward: 5.621, loss: 0.0230, took: 32.7172s\n",
            "  Batch 399/1954, reward: 5.619, loss: -0.0793, took: 32.6003s\n",
            "  Batch 499/1954, reward: 5.614, loss: 0.0140, took: 32.7697s\n",
            "  Batch 599/1954, reward: 5.628, loss: 0.0044, took: 34.8925s\n",
            "  Batch 699/1954, reward: 5.616, loss: 0.0252, took: 32.6247s\n",
            "  Batch 799/1954, reward: 5.604, loss: 0.0022, took: 32.5331s\n",
            "  Batch 899/1954, reward: 5.604, loss: -0.0200, took: 32.6194s\n",
            "  Batch 999/1954, reward: 5.601, loss: 0.0028, took: 32.5882s\n",
            "  Batch 1099/1954, reward: 5.606, loss: 0.0347, took: 32.5917s\n",
            "  Batch 1199/1954, reward: 5.597, loss: -0.0655, took: 34.8174s\n",
            "  Batch 1299/1954, reward: 5.610, loss: -0.0555, took: 32.4480s\n",
            "  Batch 1399/1954, reward: 5.603, loss: -0.0108, took: 32.8244s\n",
            "  Batch 1499/1954, reward: 5.609, loss: -0.0394, took: 32.7609s\n",
            "  Batch 1599/1954, reward: 5.604, loss: -0.0693, took: 32.5534s\n",
            "  Batch 1699/1954, reward: 5.599, loss: 0.0088, took: 32.6132s\n",
            "  Batch 1799/1954, reward: 5.604, loss: -0.0381, took: 32.7478s\n",
            "  Batch 1899/1954, reward: 5.596, loss: 0.0195, took: 34.9477s\n",
            "Mean epoch loss/reward: -0.0173, 5.6104, 5.5259, took: 649.9717s (33.0112s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.589, loss: -0.0373, took: 32.6417s\n",
            "  Batch 199/1954, reward: 5.602, loss: -0.0302, took: 32.8504s\n",
            "  Batch 299/1954, reward: 5.592, loss: -0.0499, took: 32.7032s\n",
            "  Batch 399/1954, reward: 5.589, loss: -0.0485, took: 32.5021s\n",
            "  Batch 499/1954, reward: 5.599, loss: -0.0045, took: 32.4942s\n",
            "  Batch 599/1954, reward: 5.591, loss: -0.0157, took: 34.7559s\n",
            "  Batch 699/1954, reward: 5.597, loss: -0.0028, took: 32.6456s\n",
            "  Batch 799/1954, reward: 5.616, loss: -0.0452, took: 32.6726s\n",
            "  Batch 899/1954, reward: 5.590, loss: 0.0028, took: 32.8142s\n",
            "  Batch 999/1954, reward: 5.603, loss: -0.0032, took: 32.8766s\n",
            "  Batch 1099/1954, reward: 5.602, loss: 0.0004, took: 32.6067s\n",
            "  Batch 1199/1954, reward: 5.592, loss: -0.0172, took: 32.5607s\n",
            "  Batch 1299/1954, reward: 5.577, loss: -0.0148, took: 34.8691s\n",
            "  Batch 1399/1954, reward: 5.582, loss: -0.0725, took: 32.4605s\n",
            "  Batch 1499/1954, reward: 5.587, loss: 0.0004, took: 32.7996s\n",
            "  Batch 1599/1954, reward: 5.600, loss: -0.0126, took: 32.9563s\n",
            "  Batch 1699/1954, reward: 5.583, loss: 0.0150, took: 33.1045s\n",
            "  Batch 1799/1954, reward: 5.600, loss: -0.0383, took: 33.2799s\n",
            "  Batch 1899/1954, reward: 5.584, loss: -0.0296, took: 32.7812s\n",
            "Mean epoch loss/reward: -0.0212, 5.5934, 5.5236, took: 650.9407s (32.9671s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.596, loss: 0.0109, took: 32.6592s\n",
            "  Batch 199/1954, reward: 5.578, loss: -0.0033, took: 32.4896s\n",
            "  Batch 299/1954, reward: 5.576, loss: -0.0102, took: 32.3899s\n",
            "  Batch 399/1954, reward: 5.599, loss: -0.0411, took: 32.4529s\n",
            "  Batch 499/1954, reward: 5.583, loss: 0.0094, took: 32.4067s\n",
            "  Batch 599/1954, reward: 5.579, loss: 0.0408, took: 32.5187s\n",
            "  Batch 699/1954, reward: 5.578, loss: -0.0140, took: 34.3680s\n",
            "  Batch 799/1954, reward: 5.576, loss: -0.0426, took: 32.4549s\n",
            "  Batch 899/1954, reward: 5.595, loss: 0.0356, took: 32.2554s\n",
            "  Batch 999/1954, reward: 5.581, loss: 0.0334, took: 32.2876s\n",
            "  Batch 1099/1954, reward: 5.578, loss: -0.0110, took: 32.1090s\n",
            "  Batch 1199/1954, reward: 5.574, loss: -0.0282, took: 32.4159s\n",
            "  Batch 1299/1954, reward: 5.580, loss: -0.0630, took: 32.4419s\n",
            "  Batch 1399/1954, reward: 5.574, loss: -0.0510, took: 34.7101s\n",
            "  Batch 1499/1954, reward: 5.564, loss: 0.0080, took: 32.4346s\n",
            "  Batch 1599/1954, reward: 5.574, loss: -0.0748, took: 32.1660s\n",
            "  Batch 1699/1954, reward: 5.571, loss: 0.0286, took: 32.4559s\n",
            "  Batch 1799/1954, reward: 5.575, loss: -0.0041, took: 32.2181s\n",
            "  Batch 1899/1954, reward: 5.569, loss: 0.0249, took: 32.3628s\n",
            "Mean epoch loss/reward: -0.0092, 5.5783, 5.5003, took: 641.8304s (32.6104s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.564, loss: 0.0140, took: 34.5932s\n",
            "  Batch 199/1954, reward: 5.579, loss: -0.0338, took: 32.4471s\n",
            "  Batch 299/1954, reward: 5.572, loss: -0.0500, took: 32.3540s\n",
            "  Batch 399/1954, reward: 5.576, loss: -0.0404, took: 32.2041s\n",
            "  Batch 499/1954, reward: 5.570, loss: -0.0238, took: 32.2263s\n",
            "  Batch 599/1954, reward: 5.569, loss: -0.0164, took: 32.0872s\n",
            "  Batch 699/1954, reward: 5.577, loss: -0.0186, took: 32.3041s\n",
            "  Batch 799/1954, reward: 5.557, loss: 0.0019, took: 34.2201s\n",
            "  Batch 899/1954, reward: 5.570, loss: 0.0049, took: 32.1501s\n",
            "  Batch 999/1954, reward: 5.560, loss: -0.0625, took: 32.3519s\n",
            "  Batch 1099/1954, reward: 5.562, loss: 0.0034, took: 32.2135s\n",
            "  Batch 1199/1954, reward: 5.567, loss: -0.0748, took: 32.1370s\n",
            "  Batch 1299/1954, reward: 5.555, loss: -0.0232, took: 32.1586s\n",
            "  Batch 1399/1954, reward: 5.567, loss: -0.0005, took: 32.2170s\n",
            "  Batch 1499/1954, reward: 5.559, loss: -0.0710, took: 34.1270s\n",
            "  Batch 1599/1954, reward: 5.565, loss: -0.0355, took: 32.1092s\n",
            "  Batch 1699/1954, reward: 5.553, loss: -0.0370, took: 32.2619s\n",
            "  Batch 1799/1954, reward: 5.555, loss: -0.0255, took: 32.0439s\n",
            "  Batch 1899/1954, reward: 5.564, loss: -0.0720, took: 32.0907s\n",
            "Mean epoch loss/reward: -0.0305, 5.5650, 5.4983, took: 640.3919s (32.5419s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.552, loss: -0.0472, took: 32.0707s\n",
            "  Batch 199/1954, reward: 5.560, loss: -0.0776, took: 34.2185s\n",
            "  Batch 299/1954, reward: 5.564, loss: -0.0759, took: 31.9900s\n",
            "  Batch 399/1954, reward: 5.568, loss: -0.0430, took: 32.3117s\n",
            "  Batch 499/1954, reward: 5.571, loss: -0.0365, took: 32.2211s\n",
            "  Batch 599/1954, reward: 5.554, loss: -0.0792, took: 32.1980s\n",
            "  Batch 699/1954, reward: 5.570, loss: -0.0362, took: 32.1730s\n",
            "  Batch 799/1954, reward: 5.554, loss: -0.0422, took: 32.0945s\n",
            "  Batch 899/1954, reward: 5.551, loss: -0.0679, took: 34.1318s\n",
            "  Batch 999/1954, reward: 5.546, loss: 0.0080, took: 32.2624s\n",
            "  Batch 1099/1954, reward: 5.556, loss: -0.0589, took: 32.1039s\n",
            "  Batch 1199/1954, reward: 5.545, loss: -0.0107, took: 32.1539s\n",
            "  Batch 1299/1954, reward: 5.563, loss: 0.0024, took: 32.1257s\n",
            "  Batch 1399/1954, reward: 5.563, loss: -0.0193, took: 32.0495s\n",
            "  Batch 1499/1954, reward: 5.563, loss: -0.0235, took: 32.4376s\n",
            "  Batch 1599/1954, reward: 5.560, loss: -0.1184, took: 33.6597s\n",
            "  Batch 1699/1954, reward: 5.566, loss: -0.0743, took: 31.9262s\n",
            "  Batch 1799/1954, reward: 5.560, loss: -0.0381, took: 31.9399s\n",
            "  Batch 1899/1954, reward: 5.560, loss: -0.0255, took: 32.0872s\n",
            "Mean epoch loss/reward: -0.0431, 5.5591, 5.4949, took: 638.3349s (32.4292s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.559, loss: 0.0154, took: 32.2714s\n",
            "  Batch 199/1954, reward: 5.553, loss: 0.0204, took: 31.8977s\n",
            "  Batch 299/1954, reward: 5.549, loss: -0.0186, took: 34.0060s\n",
            "  Batch 399/1954, reward: 5.546, loss: -0.0471, took: 32.1900s\n",
            "  Batch 499/1954, reward: 5.557, loss: -0.0201, took: 32.0420s\n",
            "  Batch 599/1954, reward: 5.539, loss: -0.0467, took: 32.2132s\n",
            "  Batch 699/1954, reward: 5.550, loss: -0.0193, took: 32.1424s\n",
            "  Batch 799/1954, reward: 5.556, loss: -0.0350, took: 32.2236s\n",
            "  Batch 899/1954, reward: 5.557, loss: -0.0070, took: 32.3470s\n",
            "  Batch 999/1954, reward: 5.558, loss: -0.0855, took: 34.1990s\n",
            "  Batch 1099/1954, reward: 5.545, loss: 0.0117, took: 32.2703s\n",
            "  Batch 1199/1954, reward: 5.563, loss: -0.0365, took: 32.3935s\n",
            "  Batch 1299/1954, reward: 5.547, loss: -0.0596, took: 32.2972s\n",
            "  Batch 1399/1954, reward: 5.558, loss: -0.0824, took: 32.1977s\n",
            "  Batch 1499/1954, reward: 5.548, loss: -0.0251, took: 32.3536s\n",
            "  Batch 1599/1954, reward: 5.559, loss: -0.0216, took: 33.9116s\n",
            "  Batch 1699/1954, reward: 5.537, loss: -0.0473, took: 31.9240s\n",
            "  Batch 1799/1954, reward: 5.547, loss: -0.0104, took: 32.1902s\n",
            "  Batch 1899/1954, reward: 5.533, loss: -0.0155, took: 32.0767s\n",
            "Mean epoch loss/reward: -0.0273, 5.5505, 5.4707, took: 639.2091s (32.4814s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.545, loss: -0.0142, took: 32.1224s\n",
            "  Batch 199/1954, reward: 5.548, loss: -0.0108, took: 32.1440s\n",
            "  Batch 299/1954, reward: 5.540, loss: -0.0298, took: 32.3652s\n",
            "  Batch 399/1954, reward: 5.550, loss: -0.0671, took: 33.8398s\n",
            "  Batch 499/1954, reward: 5.553, loss: -0.0113, took: 32.0300s\n",
            "  Batch 599/1954, reward: 5.548, loss: -0.0045, took: 32.1890s\n",
            "  Batch 699/1954, reward: 5.546, loss: -0.0551, took: 32.1118s\n",
            "  Batch 799/1954, reward: 5.546, loss: -0.0393, took: 32.0215s\n",
            "  Batch 899/1954, reward: 5.540, loss: -0.0125, took: 32.0080s\n",
            "  Batch 999/1954, reward: 5.541, loss: -0.0414, took: 33.9897s\n",
            "  Batch 1099/1954, reward: 5.539, loss: -0.0298, took: 32.2218s\n",
            "  Batch 1199/1954, reward: 5.550, loss: -0.0388, took: 31.9767s\n",
            "  Batch 1299/1954, reward: 5.538, loss: 0.0041, took: 32.2333s\n",
            "  Batch 1399/1954, reward: 5.546, loss: -0.0519, took: 32.3269s\n",
            "  Batch 1499/1954, reward: 5.540, loss: -0.0262, took: 32.3984s\n",
            "  Batch 1599/1954, reward: 5.541, loss: -0.0434, took: 32.2226s\n",
            "  Batch 1699/1954, reward: 5.531, loss: -0.0188, took: 34.0519s\n",
            "  Batch 1799/1954, reward: 5.531, loss: -0.0909, took: 31.9427s\n",
            "  Batch 1899/1954, reward: 5.534, loss: -0.0428, took: 31.9952s\n",
            "Mean epoch loss/reward: -0.0340, 5.5426, 5.4604, took: 638.3853s (32.4311s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.544, loss: -0.0057, took: 32.1245s\n",
            "  Batch 199/1954, reward: 5.536, loss: -0.0513, took: 32.0136s\n",
            "  Batch 299/1954, reward: 5.548, loss: -0.0099, took: 32.1478s\n",
            "  Batch 399/1954, reward: 5.540, loss: -0.0318, took: 34.0833s\n",
            "  Batch 499/1954, reward: 5.541, loss: -0.0413, took: 32.0387s\n",
            "  Batch 599/1954, reward: 5.546, loss: 0.0014, took: 32.2106s\n",
            "  Batch 699/1954, reward: 5.527, loss: -0.0039, took: 32.0706s\n",
            "  Batch 799/1954, reward: 5.536, loss: -0.0387, took: 32.1409s\n",
            "  Batch 899/1954, reward: 5.537, loss: -0.0099, took: 32.3849s\n",
            "  Batch 999/1954, reward: 5.534, loss: -0.0109, took: 32.3931s\n",
            "  Batch 1099/1954, reward: 5.541, loss: -0.1171, took: 34.2402s\n",
            "  Batch 1199/1954, reward: 5.524, loss: -0.0641, took: 32.2079s\n",
            "  Batch 1299/1954, reward: 5.535, loss: -0.0452, took: 32.1155s\n",
            "  Batch 1399/1954, reward: 5.532, loss: -0.0198, took: 32.0821s\n",
            "  Batch 1499/1954, reward: 5.537, loss: -0.0024, took: 32.2763s\n",
            "  Batch 1599/1954, reward: 5.527, loss: -0.0380, took: 32.2999s\n",
            "  Batch 1699/1954, reward: 5.536, loss: -0.0175, took: 32.3566s\n",
            "  Batch 1799/1954, reward: 5.535, loss: -0.0171, took: 34.1491s\n",
            "  Batch 1899/1954, reward: 5.540, loss: 0.0437, took: 32.0645s\n",
            "Mean epoch loss/reward: -0.0263, 5.5369, 5.5096, took: 639.6328s (32.4947s / 100 batches)\n",
            "\n",
            "  Batch 99/1954, reward: 5.540, loss: -0.0166, took: 32.4655s\n",
            "  Batch 199/1954, reward: 5.523, loss: -0.0490, took: 32.0667s\n",
            "  Batch 299/1954, reward: 5.525, loss: -0.0287, took: 32.2198s\n",
            "  Batch 399/1954, reward: 5.525, loss: 0.0236, took: 32.2740s\n",
            "  Batch 499/1954, reward: 5.534, loss: -0.0107, took: 33.9175s\n",
            "  Batch 599/1954, reward: 5.539, loss: 0.0716, took: 32.0665s\n",
            "  Batch 699/1954, reward: 5.543, loss: 0.0238, took: 32.1871s\n",
            "  Batch 799/1954, reward: 5.537, loss: -0.0597, took: 32.0095s\n",
            "  Batch 899/1954, reward: 5.536, loss: -0.0168, took: 32.1785s\n",
            "  Batch 999/1954, reward: 5.525, loss: -0.0369, took: 32.0445s\n",
            "  Batch 1099/1954, reward: 5.530, loss: -0.0532, took: 32.3046s\n",
            "  Batch 1199/1954, reward: 5.521, loss: 0.0230, took: 34.1662s\n",
            "  Batch 1299/1954, reward: 5.535, loss: -0.0006, took: 32.1310s\n",
            "  Batch 1399/1954, reward: 5.530, loss: -0.0133, took: 32.1951s\n",
            "  Batch 1499/1954, reward: 5.527, loss: 0.0072, took: 32.1486s\n",
            "  Batch 1599/1954, reward: 5.519, loss: 0.0258, took: 32.3594s\n",
            "  Batch 1699/1954, reward: 5.531, loss: -0.0219, took: 31.9874s\n",
            "  Batch 1799/1954, reward: 5.526, loss: 0.0210, took: 31.9626s\n",
            "  Batch 1899/1954, reward: 5.511, loss: 0.0007, took: 34.0423s\n",
            "Mean epoch loss/reward: -0.0055, 5.5294, 5.4682, took: 638.8992s (32.4593s / 100 batches)\n",
            "\n",
            "Average tour length:  5.460777401924133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZR5g9es4W0h",
        "outputId": "a93df3e6-ef09-409f-b0f6-e094b74092f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0pV8jKJ6LyV",
        "outputId": "d1fdfdda-798f-4ec5-8ed6-664b5b4c59c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main.py  Models  README.md  Tasks  test  vrp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cp vrp/ /content/drive/MyDrive/VRP_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbMI3pWB6RT9",
        "outputId": "68e14292-4eb9-4829-e3ba-f5ed42102f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: -r not specified; omitting directory 'vrp/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xXfUp7IC6roj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}